## ä»‹ç»
- åœ¨å½“ä»Šä¿¡æ¯çˆ†ç‚¸çš„æ—¶ä»£ï¼Œäººä»¬å¯¹äºé£Ÿè°±å’Œçƒ¹é¥ªæŠ€å·§çš„éœ€æ±‚è¶Šæ¥è¶Šé«˜ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„é£Ÿè°±æŸ¥è¯¢æ–¹å¼å¾€å¾€ç¹çä¸”ä¸å¤Ÿæ™ºèƒ½åŒ–ã€‚ä¸ºäº†æ»¡è¶³ç”¨æˆ·å¯¹é£Ÿè°±çš„å¿«é€Ÿã€å‡†ç¡®æŸ¥è¯¢éœ€æ±‚ï¼Œæˆ‘ä»¬å¼€å‘äº†åŸºäº InternLM çš„æ™ºèƒ½é£Ÿè°±é—®ç­”åŠ©æ‰‹ã€‚
- InternLM æ˜¯ä¸€æ¬¾åœ¨è¿‡ä¸‡äº¿æ ‡è®°æ•°æ®ä¸Šè®­ç»ƒçš„å¤šè¯­åƒäº¿å‚æ•°åŸºåº§æ¨¡å‹ã€‚å®ƒé€šè¿‡å¤šé˜¶æ®µçš„æ¸è¿›å¼è®­ç»ƒï¼Œå…·å¤‡è¾ƒé«˜çš„çŸ¥è¯†æ°´å¹³ï¼Œåœ¨ä¸­è‹±æ–‡é˜…è¯»ç†è§£ã€æ¨ç†ä»»åŠ¡ç­‰éœ€è¦å¼ºæ€ç»´èƒ½åŠ›çš„åœºæ™¯ä¸‹è¡¨ç°ä¼˜ç§€ã€‚æ­¤å¤–ï¼ŒInternLM è¿˜å¯ä»¥åœ¨ä¸äººç±»å¯¹è¯æ—¶å“åº”å¤æ‚æŒ‡ä»¤ï¼Œå¹¶ä¸”è¡¨ç°å‡ºç¬¦åˆäººç±»é“å¾·ä¸ä»·å€¼è§‚çš„å›å¤ã€‚
- æˆ‘ä»¬çš„æ™ºèƒ½é£Ÿè°±é—®ç­”åŠ©æ‰‹åŸºäº InternLM å¯¹è¯æ¨¡å‹ï¼Œä½¿ç”¨äº† XiaChuFang Recipe Corpus æä¾›çš„ 1,520,327 ç§ä¸­å›½é£Ÿè°±è¿›è¡Œå¾®è°ƒã€‚è¿™ä¸ªé¡¹ç›®æ­£åœ¨ä¸æ–­å¼€å‘ä¸­ï¼Œæä¾›çš„å›ç­”ä»…ä¾›å‚è€ƒï¼Œä¸ä½œä¸ºæ­£å¼èœè°±çš„çœŸå®åˆ¶ä½œæ­¥éª¤ã€‚
- é€šè¿‡è¿™ä¸€é¡¹ç›®ï¼Œæˆ‘ä»¬æ—¨åœ¨ä¸ºç”¨æˆ·æä¾›æ™ºèƒ½ã€ä¾¿æ·çš„é£Ÿè°±æŸ¥è¯¢æœåŠ¡ï¼Œè®©å¤§å®¶åœ¨çƒ¹é¥ªç¾é£Ÿçš„è¿‡ç¨‹ä¸­æ›´åŠ å¾—å¿ƒåº”æ‰‹ã€‚

### å®‰è£…

1. å‡†å¤‡ Python è™šæ‹Ÿç¯å¢ƒï¼š

   ```bash
   conda create -n xtunernew python=3.10 -y
   conda activate xtunernew
   ```

2. å…‹éš†è¯¥ä»“åº“ï¼š

   ```shell
   git clone https://github.com/wuyue2247/Recipe_Q-A_Assistant.git
   cd ./Recipe_Q-A_Assistant
   ```

3. å®‰è£…Pytorchå’Œä¾èµ–åº“ï¼š

   ```shell
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
   pip install -r requirements.txt
   ```



### è®­ç»ƒ

â€‹		æœ¬é¡¹ç›®æ¨¡å‹ ä½¿ç”¨ xtuner0.1.13 è®­ç»ƒï¼Œåœ¨ internlm2-chat-7b ä¸Šè¿›è¡Œå¾®è°ƒï¼Œ
(https://www.modelscope.cn/models/wuyue2247/Recipe_Q-A_Assistant/summary)

1. å¾®è°ƒæ–¹æ³•å¦‚ä¸‹

   ```shell
   xtuner train ${YOUR_CONFIG} --deepspeed deepspeed_zero2
   ```

   - `--deepspeed` è¡¨ç¤ºä½¿ç”¨ [DeepSpeed](https://github.com/microsoft/DeepSpeed) ğŸš€ æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚XTuner å†…ç½®äº†å¤šç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ ZeRO-1ã€ZeRO-2ã€ZeRO-3 ç­‰ã€‚å¦‚æœç”¨æˆ·æœŸæœ›å…³é—­æ­¤åŠŸèƒ½ï¼Œè¯·ç›´æ¥ç§»é™¤æ­¤å‚æ•°ã€‚

2. å°†ä¿å­˜çš„ `.pth` æ¨¡å‹ï¼ˆå¦‚æœä½¿ç”¨çš„DeepSpeedï¼Œåˆ™å°†ä¼šæ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼‰è½¬æ¢ä¸º LoRA æ¨¡å‹ï¼š

   ```shell
   export MKL_SERVICE_FORCE_INTEL=1
   xtuner convert pth_to_hf ${YOUR_CONFIG} ${PTH} ${LoRA_PATH}
   ```

   3.å°†LoRAæ¨¡å‹åˆå¹¶å…¥ HuggingFace æ¨¡å‹ï¼š

```shell
xtuner convert merge ${Base_PATH} ${LoRA_PATH} ${SAVE_PATH}
```



### å¯¹è¯

```shell
xtuner chat ${SAVE_PATH} [optional arguments]
```

å‚æ•°ï¼š

- `--prompt-template`: ä½¿ç”¨  internlm2_chatã€‚
- `--system`: æŒ‡å®šå¯¹è¯çš„ç³»ç»Ÿå­—æ®µã€‚
- `--bits {4,8,None}`: æŒ‡å®š LLM çš„æ¯”ç‰¹æ•°ã€‚é»˜è®¤ä¸º fp16ã€‚
- `--no-streamer`: æ˜¯å¦ç§»é™¤ streamerã€‚
- `--top`: å»ºè®®ä¸º0.8ã€‚
- `--temperature`: å»ºè®®ä¸º0.8ã€‚
- `--repetition-penalty`: å»ºè®®ä¸º1.002ã€‚
- æ›´å¤šä¿¡æ¯ï¼Œè¯·æ‰§è¡Œ `xtuner chat -h` æŸ¥çœ‹ã€‚


```shell
import torch
from modelscope import AutoTokenizer, AutoModelForCausalLM
from tools.transformers.interface import GenerationConfig, generate_interactive

model_name_or_path = "wuyue2247/Recipe_Q-A_Assistant" 

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')
model = model.eval()

messages = []
generation_config = GenerationConfig(max_length=max_length, top_p=0.8, temperature=0.8, repetition_penalty=1.002)

response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
response, history = model.chat(tokenizer, "é…¸èœé±¼æ€ä¹ˆåš", history=history)
print(response)
